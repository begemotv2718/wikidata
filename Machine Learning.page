---
categories: ML, mathematics, NN
...

# Natural Language processing

See [Language processing](), [Neural network and finite automata](), [Word2vec]()

* [Quora What-are-the-most-important-research-papers-which-all-NLP-students-should-definitely-read](https://www.quora.com/What-are-the-most-important-research-papers-which-all-NLP-students-should-definitely-read)

## Chat bots

* [Personalityforge](https://www.personalityforge.com/chatbot-platform.php)
* [Chatbot101](https://www.chatbots.org/ai_zone/viewthread/492/)
* [lovedroids](http://www.lovedroids.com/)
* [Journalist-like account](https://www.technologyreview.com/s/603936/three-weeks-with-a-chatbot-and-ive-made-a-new-friend/)
* [Neural conversational model: arxiv](https://arxiv.org/abs/1506.05869)

# Speech recognition

[See Speech recognition](Speech recognition)

# Speech synthesis

* [WaveNet: A Generative Model for Raw Audio](https://arxiv.org/pdf/1609.03499.pdf) [popular blog entry](https://deepmind.com/blog/wavenet-generative-model-raw-audio/) github implementations: [tensorflow](https://github.com/ibab/tensorflow-wavenet), [keras](https://github.com/basveeling/wavenet)
* [Нейросетевой синтез речи своими руками / Блог компании Центр речевых технологий (ЦРТ) / Хабр](https://habr.com/company/speechpro/blog/358816/)


# Machine learning for antisocial elements like me

* [ROCSpeak: Using machine learning analysis to help people improve communication skills](http://www.cs.rochester.edu/news-events/news/2016-06-16_mhoque-microsoft.html)
* [The same guy](http://www.rochester.edu/newscenter/ehsan-hoque-an-mit-innovator-under-35-177152/)
* Can be tried [online](https://www.machinteraction.com/rocspeak/)

# Papers

## Arxiv

* [Explaining predictions of any classifier](https://arxiv.org/abs/1602.04938)
     * [Examples on NSFW images: habr](https://habrahabr.ru/post/282071/)
         * [Deconvnets: explaining predictions](https://arxiv.org/abs/1311.2901)
     * [Git repository of the author of the paper](https://github.com/marcotcr?tab=repositories) [More precisely](https://github.com/marcotcr/lime-experiments) [Lime itself](https://github.com/marcotcr/lime)
     * [Anchor: newer model](https://github.com/marcotcr/anchor)
* [Generating posters for papers: arxiv](https://arxiv.org/abs/1604.01219)
* [Why deep and cheap learning work so well: Tegmark](https://arxiv.org/abs/1608.08225)
    * [Response of renorm-group guys](https://arxiv.org/abs/1609.03541)
    * See more about this under [Renormalization group]()
* [Face aging with adversarial networks](https://arxiv.org/abs/1702.01983)
* [Food steganography](https://arxiv.org/abs/1704.03330) [patent application](http://www.freshpatents.com/-dt20150305ptan20150059438.php)
* [Image2calories](https://www.cs.ubc.ca/~murphyk/Papers/im2calories_iccv15.pdf)
* [Machine supervised machine learning](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html)
* [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/pdf/1604.06174.pdf) [Github: Saving memory using gradient-checkpointing](https://github.com/openai/gradient-checkpointing)
* [Neural Turing Machines](https://arxiv.org/pdf/1410.5401.pdf) Idea: try to achieve von Neuman computing with trainable neural networks. They try to generalize RNN in the same way as Turing complete machines generalize finite state automatons (FSA). 
* [1802.07896 L2-Nonexpansive Neural Networks](https://arxiv.org/abs/1802.07896) -- Neural networks robust to adversarial examples with small disturbances
* [[1203.1513] Invariant Scattering Convolution Networks](https://arxiv.org/abs/1203.1513) -- Neural networks maximally invariant to translations, rotations, etc. Wavelets based
* [A mixed-scale dense convolutional neural network for image analysis | PNAS](http://www.pnas.org/content/115/2/254) -- authors propose a simple uniform architecture for conv networks for image processing

## Tools

* [GitXiv](http://www.gitxiv.com/) -- mapping between arxiv.org and github repositories.
* [figshare - credit for all your research](https://figshare.com/browse) -- materials for various papers

## Other

* [Deep face recognition](http://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/parkhi15.pdf)
* [Facial expression recognition with CNN](http://cs231n.stanford.edu/reports/2016/pdfs/023_Report.pdf)
* [Recycle-GAN: transfer pose from one video to another](https://www.cs.cmu.edu/~aayushb/Recycle-GAN/)

## Naftali Tishby

* [Home page](http://naftali-tishby.strikingly.com/)
* [The information bottleneck method](https://arxiv.org/pdf/physics/0004057.pdf)
* [Stanford seminar: Youtube](https://www.youtube.com/watch?v=XL07WEc2TRI)
* [Yandex seminar](https://www.youtube.com/watch?v=FSfN2K3tnJU)
* Quanta magazine
    * [Deep Learning Relies on Renormalization, Physicists Find | Quanta Magazine](https://www.quantamagazine.org/deep-learning-relies-on-renormalization-physicists-find-20141204/)
    * [New Theory Cracks Open the Black Box of Deep Learning | Quanta Magazine](https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/)

# Recipies and tutorials

* [How to approach almost any ML problem: Abhishek Thakur, Kaggle winner](http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/)
* [Optimization of NN review: habr](https://habrahabr.ru/post/318970/)
* [Y.Bengio Practical Recommendations for Gradient-Based Training of Deep Architectures](https://arxiv.org/pdf/1206.5533v2.pdf)
* [Kaggle word2vec tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial)
* [NN from scratch: wildml](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)
* [Yann LeCun answer on Quora about best tutorials](http://qr.ae/T5jIng)
* [habr paper on Kaggle competition problem (click prediction)](https://habrahabr.ru/post/323938/)
* [another habr click prediction](https://habrahabr.ru/post/254151/) [reference to code](https://github.com/Ivanopolo/Avazu_Code)
* [3 key steps to building predictive app](https://www.datanami.com/2015/03/03/the-3-key-steps-to-building-a-predictive-app-with-machine-learning/?utm_source=feedly&utm_reader=feedly&utm_medium=rss&utm_campaign=the-3-key-steps-to-building-a-predictive-app-with-machine-learning)
* [Autoencoders](http://www.kdnuggets.com/2015/03/deep-learning-curse-dimensionality-autoencoders.html) -- mostly nothing interesting, explains why it is important to reduce dimensionality
* XGBoost
    * [XGboost with Scikit-learn](http://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)
    * [Kaggle gradient boosting](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)

* [Scikit-learn choosing the right estimator](http://scikit-learn.org/dev/tutorial/machine_learning_map/index.html)
* Data augmentation
    * [Data Augmentation | How to use Deep Learning when you have Limited Data — Part 2](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)
    * [Introduction to Dataset Augmentation and Expansion - Algorithmia Blog](https://blog.algorithmia.com/introduction-to-dataset-augmentation-and-expansion/)
    * [Kaggle tutorial](https://www.kaggle.com/dansbecker/data-augmentation)

* Model size reduction
    * [Deep learning model compression](https://medium.com/comet-app/deep-learning-model-compression-for-image-analysis-methods-and-architectures-398f82b0c06f)
    * Main paper network pruning: [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
    * [TensorFlow Model Optimization Toolkit — Pruning API](https://medium.com/tensorflow/tensorflow-model-optimization-toolkit-pruning-api-42cac9157a6a)

* Style transfer
    * [habr: Style transfer for generating russian fonts](https://habr.com/ru/post/325510/)

* [5agado/data-science-learning: Repository of code and resources related to different data science and machine learning topics. For learning, practice and teaching purposes.](https://github.com/5agado/data-science-learning)
* Image upscaling networks
    * [Game and films upscaling](https://www.reddit.com/r/GameUpscale/comments/a7msxo/getting_started/)

* [Graduate summer school: deep learning](https://www.ipam.ucla.edu/programs/summer-schools/graduate-summer-school-deep-learning-feature-learning/?tab=schedule)

## Tricks

* [Woodbury matrix identity - Wikipedia](https://en.wikipedia.org/wiki/Woodbury_matrix_identity) Allow to quickly recalculate inverse of matrix when only few row change.
* [What are some relatively unknown but powerful data science / machine learning tricks? - Quora](https://www.quora.com/What-are-some-relatively-unknown-but-powerful-data-science-machine-learning-tricks)

# Books

* [Математические методы обучения по прецедентам Vorontsov](http://www.machinelearning.ru/wiki/images/6/6d/Voron-ML-1.pdf)
* [Deep learning book](http://www.deeplearningbook.org/) Generated using [pdf2htmlEX](https://github.com/coolwanglu/pdf2htmlEX/wiki)
* [Introduction to machine learning interpretability](https://pages.dataiku.com/hubfs/ML-interperatability.pdf) Free book on interpretability, but not very detailed

## Conventional statistics

* [Statistical inference book](https://fsalamri.files.wordpress.com/2015/02/casella_berger_statistical_inference1.pdf)


# ML Debug

* [Debugging ML models](https://nlpers.blogspot.com.by/2016/08/debugging-machine-learning.html)
* [37 reasons your neural network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)

# Tensorflow

* [Tensorflow]()

# Computer vision

* [Computer vision]()

# Github

* [Machine Learning Top 10 Open Source Projects](https://medium.mybridge.co/machine-learning-top-10-open-source-projects-v-feb-2018-d1d39062bd20)
* [NVIDIA open seq2seq](https://github.com/NVIDIA/OpenSeq2Seq) Machine translation, speech recognition, NLP. Tensorflow >=1.10 required.

# Faces

* [Facial expression recognition for humans](http://www.wikihow.com/Easily-Read-Faces-and-Facial-Expressions)
* [github: Face expression recognition using kaggle dataset](https://github.com/Hanzhuo/Facial-Expression-Recognition-with-TensorFlow-Convolutional-Neural-Networks) [Dataset](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)
* [OpenFace: free and open source face recognition](https://cmusatyalab.github.io/openface/)
    * [Installation](https://cmusatyalab.github.io/openface/setup/)
    * [Github](https://github.com/cmusatyalab/openface)
    * [API doc](http://openface-api.readthedocs.io/en/latest/index.html)
    * [dlib: prerequisite](https://github.com/davisking/dlib)
        * [dlib installation](https://npatta01.github.io/2015/08/10/dlib/)
    * [Torch:prerequisite](http://torch.ch/)
* [OpenCV Facerecognizer](http://docs.opencv.org/2.4.8/modules/contrib/doc/facerec/facerec_api.html)
* [habr: Modern face recognition with deep learning](https://habrahabr.ru/post/306568/)
    * [gist: Steps](https://gist.github.com/ageitgey/ddbae3b209b6344a458fa41a3cf75719)
    * [gist: step-2a finding face landmarks](https://gist.github.com/ageitgey/ae340db3e493530d5e1f9c15292e5c74#file-step-2a_finding-face-landmarks-py)
    * [gist: step-2b projecting faces](https://gist.github.com/ageitgey/82d0ea0fdb56dc93cb9b716e7ceb364b)
* Masked face recognition [Disguised Face Identification (DFI) with Facial KeyPoints using Spatial Fusion Convolutional Network](https://arxiv.org/abs/1708.09317v1)
* [Deep face recognition using imperfect facial data (also masked faces)](https://www.sciencedirect.com/science/article/pii/S0167739X18331133)
* Face generation
    * [Random face generation](http://carpedm20.github.io/faces/) somewhat Asian inclined
    * [BEGAN: Boundary Equilibrium Generative Adversarial Networks](https://arxiv.org/pdf/1703.10717.pdf) [Github tensorflow](https://github.com/carpedm20/BEGAN-tensorflow)
    * [Glow: Better Reversible Generative Models](https://blog.openai.com/glow/)
    * [Yet newer and better from NVIDIA](https://github.com/tkarras/progressive_growing_of_gans) via [Linkedin story](https://www.linkedin.com/pulse/americas-next-topbot-model-ben-taylor-deeplearning-/)
        * [And yet newer one from NVIDIA](https://docs.google.com/document/d/1SDbnM1nxLZNuwD8fQkIigUve_SlihgoCmvjN3e388Us/edit)
    * [DCGAN Tutorial — PyTorch Tutorials 1.0.0.dev20181002 documentation](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
* Age and gender
    * [impl1](https://github.com/GilLevi/AgeGenderDeepLearning)
    * [impl_orig](http://www.openu.ac.il/home/hassner/projects/cnn_agegender/)
    * [impl_tensorflow](https://github.com/dpressel/rude-carnie)
* [The Code for Facial Identity in the Primate Brain - CaltechAUTHORS](https://authors.library.caltech.edu/77942/)

# Whole body

* General body pose estimation from CMU [github openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
* [MPI SMPL](http://smpl.is.tue.mpg.de/downloads) Automatic generation of rigged body from pose. Animation rigging?

# Datasets, etc.

* [Data science Brain computer interface project](https://groups.google.com/forum/#!forum/data-science-bci-project) -- encephalograms
* [PlantCLEFT -- dataset of plant species -- possibly noisy](http://www.imageclef.org/lifeclef/2016/plant)
* [Electro-L sattelite images of Earth in various spectrum bands (mostly IR)](http://www.ntsomz.ru/electro/source_images) [Sattelite description](http://planet.iitp.ru/spacecraft/electro_2_rus.htm)
* [Russian open semantics: habr](https://habrahabr.ru/post/344582/) [Github](https://github.com/dkulagin/kartaslov)
* Emotion by voice [Multimodal database RAMAS (The Russian Acted Multimodal Affective Set)](http://neurodatalab.com/en/projects/RAMAS/) [RAMAS: Russian Multimodal Corpus of Dyadic Interaction for studying emotion recognition  Preprints](https://peerj.com/preprints/26688/) via [Pitch-tracking, или определение частоты основного тона в речи, на примерах алгоритмов Praat, YAAPT и YIN / Хабр](https://habr.com/company/neurodatalab/blog/416441/?utm_source=habrahabr&utm_medium=rss&utm_campaign=416441)

* Appolo mission dataset
    * [habr article](https://habr.com/company/audiomania/blog/417541/)
    * [Transcript example](https://www.jsc.nasa.gov/history/mission_trans/AS11_CM.PDF)
    * [Page with transcripts](https://www.nasa.gov/mission_pages/apollo/40th/a11_audio_db.html)
    * [Audio collection](https://archive.org/details/nasaaudiocollection)
* [Google dataset search](https://toolbox.google.com/datasetsearch) 
* [habr:Подборка датасетов для машинного обучения](https://habr.com/ru/post/452392/) few kaggle datasets, etc. Mostly images
    * [xView dataset](http://xviewdataset.org/#dataset) Sattelite images/map dataset

# Link bowls

* [Глубоко внутри глубокого обучения lj:ailev](http://ailev.livejournal.com/1254558.html)
* [Awesome AI: github](https://github.com/owainlewis/awesome-artificial-intelligence) Too few fucking code link, too many fucking philosophy links
* [Список ресурсов по машинному обучению habrahabr](https://habrahabr.ru/company/spbifmo/blog/277593/)
* [Papers we love repository](https://github.com/papers-we-love/papers-we-love) They also have a [web site](http://paperswelove.org/)

# ML and software defined radio

[Machine learning and Software defined radio]()

# Neural networks and human cognition

It is important to understand that neural network is mapped on real neuron only after "frequency modulation transform" (signals on artificial neuron corresponds to the frequency of biological neuron firing).

* [Dynamical system hypothesis in Cognitive Science](http://www.cs.indiana.edu/~port/pap/ency.dec.htm) -- see also [Patterns in art](), especially about fractals and dynamical systems
* [Cognitive science link collection](http://www.cambridge.org/features/bermudez/students/links_learning.htm)
* [Cognitive science encyclopedic article: Stanford encyclopedia of philosophy](https://plato.stanford.edu/entries/cognitive-science/)

# Microproject

IBAN links [Bank of ireland IBAN generator](https://personalbanking.bankofireland.com/help-centre/tools-and-calculators/calculate-iban/?err=0011), [see example iban per country](http://www.xe.com/ibancalculator/), [some insight into generation rules](https://www.ibancalculator.com/iban_validieren.html?&no_cache=1&tx_valIBAN_pi1[iban]=DE12500105170648489890&tx_valIBAN_pi1[fi]=fi#) https://www.ibancalculator.com/iban_validieren.html?&no_cache=1&tx_valIBAN_pi1[iban]=DE12500105170648489890&tx_valIBAN_pi1[fi]=fi# The [main site](https://www.ibancalculator.com/) has ambiguos IBAN recovery. IBAN validation [code in swift](https://github.com/readefries/IBAN-Helper). [Validation algorithm in simple english wiki](https://simple.wikipedia.org/wiki/International_Bank_Account_Number#cite_note-IBANValidation-3). [Check digit](https://en.wikipedia.org/wiki/Check_digit) [Estonian account check digit](http://www.pangaliit.ee/en/settlements-and-standards/check-digit-calculator-of-domestic-account-number). [Photocopy filter](http://www.imagemagick.org/discourse-server/viewtopic.php?t=14441) 

[Latex font selection](http://tex.stackexchange.com/questions/25249/how-do-i-use-a-particular-font-for-a-small-section-of-text-in-my-document/25251#25251) [swift bank codes](https://www.theswiftcodes.com/azerbaijan/) [perl iban module](https://metacpan.org/pod/Business::IBAN) [random iban generator (not very good)](http://www.mobilefish.com/services/random_iban_generator/random_iban_generator.php)
[Spanish account number verification](https://es.wikipedia.org/wiki/C%C3%B3digo_cuenta_cliente) [php library with iban generation](https://github.com/jschaedl/Iban)

## Conditional random fields

* [Wikipedia: Conditional random fields](https://en.wikipedia.org/wiki/Conditional_random_field)

Specifically, CRFs find applications in POS Tagging, shallow parsing,[3] named entity recognition,[4] gene finding and peptide critical functional region finding,[5] among other tasks, being an alternative to the related hidden Markov models (HMMs). In computer vision, CRFs are often used for object recognition and image segmentation.

* [CRFSuite](http://www.chokkan.org/software/crfsuite/)

## Maximum entropy Markov models

* [Wikipedia: Maximum entropy Markov model](https://en.wikipedia.org/wiki/Maximum-entropy_Markov_model)

# Recurrent Neural networks

* [RNN tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
* [Andrej Karpathy several funny RNN projects](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    * [Github page ](https://github.com/karpathy/char-rnn)
* [Seq2seq in machine translation: tensorflow example](https://www.tensorflow.org/tutorials/seq2seq) [github](https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate) [issues](https://github.com/tensorflow/tensorflow/issues/8191) [possible fix for issue](https://github.com/oxwsds/tensorflow/commit/461d5488f7fd848a3161c7ee7b0353799d7a4663#comments)
    * Project to try on basis of this: chatbot
* [Penn treebank marking with rnn: tensorflow](https://www.tensorflow.org/tutorials/recurrent)
* [Shakespeare generation](https://github.com/martin-gorner/tensorflow-rnn-shakespeare) 
    * Seems to be adaptable to bash completion task
* [Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [Google question reformulation with Seq2seq](https://ai.googleblog.com/2018/10/open-sourcing-active-question.html)
* [Neural conversational model: arxiv](https://arxiv.org/abs/1506.05869) Basic paper on chat bots using seq2seq model. Might be interesting to look for papers citing this one.

## Terminology

* LSTM cell = [Long short memory](https://en.wikipedia.org/wiki/Long_short-term_memory)
* GRU cell = [Gated recurrent unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit)

# Comparing RNN, MEMM, HMM

[Quora: What are the pros and cons of these three sequence models: MaxEnt Markov Model, Conditional random fields, and recurrent neural networks?](https://www.quora.com/What-are-the-pros-and-cons-of-these-three-sequence-models-MaxEnt-Markov-Model-Conditional-random-fields-and-recurrent-neural-networks/answer/M-Pavankumar-Reddy)

# Generative adversarial networks

* [Paper](https://arxiv.org/pdf/1406.2661v1.pdf)
* [Unsupervised Representation learning with deep convolutional generative adversarial networks](https://arxiv.org/pdf/1511.06434.pdf) [github](https://github.com/Newmu/dcgan_code) [tensorflow implementation of DCGAN](https://github.com/carpedm20/DCGAN-tensorflow)
    * FaceApp seems to use something like this (google Yaroslav Goncharov)
* [Open AI: some nice examples](https://blog.openai.com/generative-models/#contributions)
* [DCGAN Tutorial — PyTorch Tutorials 1.0.0.dev20181002 documentation](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
* Tricks to improve GAN training [Ganhacks github](https://github.com/soumith/ganhacks)
* [Curated list of awesome GAN applications: github](https://github.com/nashory/gans-awesome-applications)
* [AttnGAN: generate image from text](https://github.com/taoxugit/AttnGAN)
* [Spectral normalization GAN for generating images from parameters: github](https://github.com/pfnet-research/sngan_projection)

## Image completion with GANs

* [Locally and globally consistent image completion](http://hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/) No code for discriminator
* Simpler examples
    * Generative face completion [paper](https://drive.google.com/file/d/0B8_MZ8a8aoSeenVrYkpCdnFRVms/edit) [github](https://github.com/Yijunmaverick/GenerativeFaceCompletion)

# Convolutional neural networks

* [Github: convolutional arithmetics](https://github.com/vdumoulin/conv_arithmetic) Animations how different types of convolution work.

# Clustering

* [Three Popular Clustering Methods and When to Use Each](https://medium.com/predict/three-popular-clustering-methods-and-when-to-use-each-4227c80ba2b6)


# Blogs

* [Andrei Karpathy blog](http://karpathy.github.io/2015/11/14/ai/)

# Image search

* [GNU content based image search: gift](https://www.gnu.org/software/gift/)

# Reinforcement learning

* [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)

# Hardware for ML

* [Quora: What is the best GPU to be used for Deep Learning with budget 1000 ](https://www.quora.com/What-is-the-best-GPU-to-be-used-for-Deep-Learning-with-budget-1000)
* [Quora: What is the typical computing power that needs to do well in most Kaggle competitions](https://www.quora.com/Whats-the-typical-computing-power-that-needs-to-do-well-in-most-Kaggle-competitions)

## NVIDIA CUDA

* [Installation guide](http://docs.nvidia.com/cuda/cuda-installation-guide-linux/)

## FPGA for ML

* [Xilinx: FPGA for ML](https://www.xilinx.com/applications/data-center/machine-learning.html)
* [Amazon: FPGA developer AMI](https://aws.amazon.com/marketplace/pp/B06VVYBLZZ) [README](https://s3.amazonaws.com/EULA/FPGA_README.txt)
* [github: Xilinx ML suite](https://github.com/Xilinx/ml-suite) Summary: this is a compiler of tensorflow and caffe models to intermediate representation suitable to feed into Xilinx xDNN. Seems to be runnable on AWS F1 AMI. [Amazon link](https://aws.amazon.com/marketplace/pp/B077FM2JNS)
* [Running Yolo on FPGA: issue on github](https://github.com/Xilinx/ml-suite/issues/39)
    * [pdf with instructions](https://github.com/Xilinx/ml-suite/files/2365825/yolo_on_aws_f1.pdf)

# Fun

* [habr: Poetry generation](https://habrahabr.ru/post/334046/)

# Unusual 

* [Quora: Gaussian process algorithm](https://www.quora.com/Which-is-your-favorite-Machine-Learning-algorithm/answer/Duane-Rich?srid=ibVo)
* [Probabilistic programming](http://v1.probmods.org/conditioning.html#example-reasoning-about-the-tug-of-war) Seems to be similar to probabilistic graphical models
* [Proton: Multitouch Gestures as Regular Expressions](http://vis.berkeley.edu/papers/proton/)
* [Polynomial Regression as an Alternative to Neural Nets](https://arxiv.org/pdf/1806.06850.pdf) -- this guys show that polynomial regression (being sufficiently non-linear) sometimes work no worse than a sophisticated NNs.

## Topological data analysis

This theme is introduced by Coleen Farrelly at Quora, it seems that she is quite fond of this technique. She claims that this method works extremely well on small datasets. She has review of this method in the [following paper](https://psyarxiv.com/mknpj/). I'm somewhat suspicious that she is hypnotized by very intellectual sounding of all this stuff. What we can still extract from the writing:

1. There is a number of R packages for TDA [TDA](https://lib.ugent.be/CRAN/web/packages/TDA/index.html) [TDAMapper](https://lib.ugent.be/CRAN/web/packages/TDAmapper/index.html) [TDAstats](https://lib.ugent.be/CRAN/web/packages/TDAstats/index.html)
2. The method was introduced in the following papers
    * Carlsson G (2009). “Topology and data.” Bulletin of the American Mathematical Society, 46(2), 255–308.
    * [Edelsbrunner, Harer "Persistent Homology - a survey". Contemporary mathematics, 453, 257-282](https://www.maths.ed.ac.uk/~v1ranick/papers/edelhare.pdf)
    * Chen, Genovese, Wasserman "Statistical inference using the Morse-Smale complex". Electronic Journal of Statistics 11(1), 1390-1443

## Some useful optimization techniques

* [Bayesian optimization](https://app.sigopt.com/static/pdf/SigOpt_Bayesian_Optimization_Primer.pdf) -- including optimization of other models.
    * [Bayesian optimization for ML algorithm](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf) (hyperparameter fitting)


# Tools

## Jupiter notebooks

* [Useful jupiter extensions](https://codeburst.io/jupyter-notebook-tricks-for-data-science-that-enhance-your-efficiency-95f98d3adee4)


# Personality by text

See [Psychological types]()


# ML for programming

* [ML for programming]()

# Voice command research

* [Speech recognition]()

# See also

* [Visualization]() e.g. for t-SNE
* [Image processing]()
* [Big data]()
* [Graphing]()
